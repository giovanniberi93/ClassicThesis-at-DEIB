% !TEX root = ../ClassicThesis_DEIB.tex

\chapter{Background and Tools} \label{chap:backgroundAndToolsChapter}

In this chapter we are going to describe the general concepts this thesis deals with, together with the main tools we used to address the project. Since this thesis is in the frame of \ac{GRAPE} project (see Chapter \ref{chap:grapeProject}),  most of them are typical of the robotic field and, more specifically, of the agricultural robotics. This last field should be seen in the wider context of the so-called \textit{E-agriculture}; to give a precise definition of this term, we make reference to the FAO (Food and Agriculture Organization) definition\footnote{http://www.fao.org/fileadmin/templates/rap/files/uploads/E-agriculture\_Solutions\_Forum.pdf}:
\blockquote{\textit{E-agriculture, or ICTs in agriculture, is about designing,
developing and applying innovative ways to use ICTs with a
primary focus on agriculture. E-agriculture offers a wide range
of solutions to agricultural challenges and has great potential
in promoting sustainable agriculture while protecting the
environment.}}
The \ac{GRAPE} project, that will be described with further details in chapter \ref{chap:grapeProject}, is about the design and realization of an \ac{UGV} with control and operative task in a vineyard environment, so in this chapter we'll deal with topics concerning software development in robotics, estimation of the state of a robot, autonomous navigation

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Robot Operating System}\label{sec:robotOperatingSystem}
\ac{ROS} is the \textit{robotic middleware} we used to develop the sofware components of the system described in this thesis. We decide to use it because of its great modularity, the availability of a very large number of packages, well documented APIs and an active community. Moreover, \ac{ROS} is a very widespread system, so its power and versatility are well known in the field of software development for robotics. Citing words from its offical website\footnote{http://wiki.ros.org/ROS/Introduction},
these are \ac{ROS} main features: 
\blockquote{
\textit{It provides the services you would expect from an operating system, including hardware abstraction, low-level device control, implementation of commonly-used functionality, message-passing between processes, and package management. It also provides tools and libraries for obtaining, building, writing, and running code across multiple computers}.
} 

\ac{ROS} is actually a \textit{meta-operating system}, that is, it's not an operating system in the traditional sense (it requires to be run on top of an another operating system; currently, the only officially supported OS is Linux Ubuntu), but it provides a peer-to-peer network that processes can use to create and process data together. This network is implemented through TCP, and it's called \textit{Computation Graph}. In this section, we're going to describe \ac{ROS} with more detail, with particular emphasis on the different tecniques that nodes can use to communicate among them. Keep in mind that, even if these tecniques differs a lot, they all are strongly typed \textit{i.e.} in order to define a channel (with \textit{channel} we now mean one of the tecnique that we are going to describe. It's not the name of a specific communication tool) you also need to define the types of message that are going to be exchanged throught it. \ac{ROS} already defines a lot of useful message types (\textit{e.g.} \textit{LaserScan.msg}, \textit{PoseWithCovarianceStamped.msg}), grouped by domain (\textit{e.g.}, \textit{Sensor\_msgs}, \textit{Geometry\_msgs}). However a simple message definition language is provided, and users are encouraged to define their own message types to make them as self-explanatory as possible.
\begin{description}
\item[ROS Master] Even if the Computation Graph is a peer-to-peer network, a central process, called  \textbf{\ac{ROS} Master}, is required to exist, to provide naming and registration services to all the user processes In this. Once the processes have located each other through the services offered by the Master, they can communicate peer-to-peer without involving a central entity;

\item[nodes] The processes that are in the Computation Graph are called \textbf{nodes}, and they are the atomic units of the computational graph. The \ac{ROS} API are available in C++, Python and Lisp, but C++ is the most widely used. One of the aims of \ac{ROS} is to be modular at a fine-grained scale, so a complex task should be achieved through cooperation of several different nodes, each with quite narrow tasks, rather than one large node that include all the functionalities. Nodes can use different techniques for communication, depending whether the message is a part of data stream or it is a request message (\textit{i.e.} a response message is expected) and, in this last case, on the (expected) duration and complexity of the computation of the response.


\begin{figure}
	\centering
	\subfloat[]{%
		\includegraphics[width=0.3\textwidth]{Images/background_and_tools/ROS_master_example_english_1.png}}
	\qquad
	\subfloat[]{%
		\includegraphics[width=0.25\textwidth]{Images/background_and_tools/ROS_master_example_english_2.png}}
	\subfloat[]{%
		\includegraphics[width=0.25\textwidth]{Images/background_and_tools/ROS_master_example_english_3.png}}		
		
	\caption{\textit{Three phases of the setup of communication of nodes throught topics.}}
	\label{fig:topicRegistration}
\end{figure}


\item[topics] Topics implements a \textit{publish-subscribe} paradigm, are they the easiest way that nodes can use to communicate with each other, and basically are named channels, characterized by the type of the messages that are sent through it. When a node \textit{publish} a message on a certain topic, the message is read from all the nodes that previously \textit{subscribed} to that topic, interfacing with the Master. Note that:
\begin{itemize}
	\item this technique leads to a strong decoupling between publishers and subscribers to a topic, because a publisher node is, from an high-level perspective\footnote{Actually, publisher nodes always know the list of nodes subscribed to their topics. But this  is only used in connection phase, and to avoid a situation where a node publish on a topic with no subscribers, for the sake of efficiency.},
	not even aware of the presence of subscribers, and viceversa.
	\item the relationship between publishers and subscribers is \textit{many-to-many}, \textit{i.e.} multiple nodes can publish on a topic, and multiple nodes can subscribe to a topic.
\end{itemize}
We can easily conclude that this method is very suitable for passing streams of data (\textit{e.g.} the handler of a \ac{LIDAR} streams its measurements over the network, or a node publish the velocities commands for the wheels of a robot), but there is no notion of a \textit{response} to a message, so it's not suitable for \textit{request-response} communication. 


\item[services] Services are defined by a name, and a couple of message types that describe the \textit{request} type and the \textit{response} type. Each service is offered by a Service server to any Service client that perform a call. So, Services implement an inter-node communication that is very similar to traditional function calling in most common programming languages (\textit{e.g.} C++, Java), in the sense that:
\begin{itemize}
	\item Service calls are blocking
	\item using Services, the inter-node communication is \textit{one-to-one}
\end{itemize}
These properties make Services suitable for punctual (in opposition to data stream) inter-node communication, such as: request of parameters values to another node, ask a node that handles a camera to take a picture, ask a node that perform navigation task to clear the current map.

\begin{figure}
	\centering
	\includegraphics[width=1.1\textwidth]{Images/background_and_tools/rqt_graph.png}
	\caption{\textit{An example of the \ac{ROS} Computation Graph, visualized with} rqt\textit{: nodes are represented with circles, rectangles represent topics, and arrows go from a node to a topic it publishes on, or from a topic to a node subscribed to it. It's easy to recognize the} many-to-many \textit{relationship.}}
	\label{fig:tfGraph}
\end{figure}

\item[actions] While Services, with their resemblance to traditional function calls, can address pretty well the problem of \textit{one-to-one} inter-node communication, they can be quite unsatisfying if the computation required to produce the response is demanding in term of execution time (\textit{e.g.}, navigation of a robot from one point to another in an environment), because the caller is stuck at the line with the Service invocation until the end of the procedure. Services show their weaknesses also in situations where it could be useful to observe the intermediate results of the computation triggered by the request (\textit{e.g.}, a very complex manipulation procedure). \textbf{Actions} are very suitable in this context because, at the cost of a more complex implementation, provide an asynchronous and fully preemptable remote procedure call, with the possibility of monitoring intermediate results if needed. Differently from Topics and Services, Actions are not native in \ac{ROS}, and their functionalities are built on top of the other \ac{ROS} messagging systems (See figure \ref{fig:actionlib}). Asynchronicity is provided by the use of callbacks.
\end{description}

\begin{figure}
	\centering
	\includegraphics[width=0.95\textwidth]{Images/background_and_tools/actionlib.png}
	\caption{\textit{Sketch of the implementation of actionlib, through \ac{ROS} topics and a callback system.}}
	\label{fig:actionlib}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=0.8\textwidth]{Images/background_and_tools/tf_tree.png}
	\caption{\textit{A robot in 2 different positions, with} tf \textit{frames in evidence: x-axis is red, y-axis is green, z-axis is blue. The frames are the same in both configuration, but the transformations (i.e. rototranslations) between them are different.}}
	\label{fig:tfTreeRviz}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{TF: The Transform Library}\label{sec:tf}
\textit{tf} is a \ac{ROS} library, which task is very important to understand in order not to get lost in the next sections and chapters.  The goal of \textit{tf} is:

\blockquote{
\textit{" [...] provide a standard way to keep track of coordinate frames and transform data within an entire system such that individual component users can be confident that the data is in the coordinate frame that they want without requiring knowledge of all the coordinate
frames in the system" \parencite{tfPaper}}
}
The utility of such a component is straightforward, even in quite simple robotic systems. We'll describe here a situation we stumpled upon exactly in the development of the \ac{GRAPE} project, where of the utility of \textit{tf} is very easy to understand; you'll be able to better contextualize this example after you've read Chapter \ref{chap:kinovaArmChapter}. In this example a \ac{LIDAR}, mounted on top of the final joint of a robotic arm, acquires data while the arm is moving in order to create a point cloud that will be processed later. To get a meaningful point cloud, it's mandatory to keep track of the movement of the \ac{LIDAR} with respect to a point with speed equal to zero (\textit{e.g.} the base link of the arm, or the base link of the whole robot), and this gets even more difficult because of the multiple (6 in our specific case) joints of the arm; but this problem can be easily addressed by means of \textit{tf}, that we are now going to describe with more detail. 
\textit{tf} implementation relies on \ac{ROS} topics (see Section \ref{sec:robotOperatingSystem}) and achieves the goal mentioned before by building an oriented graph where vertices are reference frames, and edges are transformations (rototranslations) between frames. \textit{tf} does not assume a constant structure and, if a path exists between two reference frames in the graph, the direct transformation between them can be computed by composition of transformation. Since, in general, multiple paths between 2 vertices can exists in a directed graph and this could lead to ambiguity in computing the transformation between two reference frames, the graph is forced to be acyclic. Discongit anected subgraphs are allowed, but of course transformation between vertices that belong to different subgraphs cannot be computed. 
The main components of the library are:
\begin{itemize}
	\item \textbf{\textit{tf} broadcasters}: they are simple software components, that publish a transformation between two reference frames every time an update is available. Different broacasters does not sync together the publishing phase
	\item \textbf{\textit{tf} listeners}: they are more complex components, because they take into account that broadcasters are not synced. Since both transformations and queries to \textit{tf} graph are stamped, listeners make use of queues to store the most recent transformations, and they  interpulate old values using SLERP (Spherical Linear intERPolation) to return a transformation for which there is no measured value at the requested timestamp.
\end{itemize}

\begin{figure}
	\centering
	\includegraphics[width=0.6\textwidth]{Images/background_and_tools/tfGraph.JPG}
	\caption{\textit{An example of} tf \textit{tree}}
	\label{fig:tfGraph}
\end{figure}

In figure \ref{fig:tfTreeRviz} you can see a graphical representation of the reference frames tracked in a Nao Robot, while figure \ref{fig:tfGraph} shows an example of \textit{tf} graph visualized with visualization framework \textit{rqt}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Odometry}\label{sec:odometry}

The problem of odometry, \textit{i.e.} estimation of the position of a robot in an environment is harder than it could seem. Formally, odometry estimation is the problem of estimating over time the tuple: 
\begin{equation}
	<x, y, z, \theta,\dot{x}, \dot{y}, \dot{z}, \dot{\theta}>
	\label{eq:odometryTuple}
\end{equation} 
given the measurement of some motion sensors. 
To better understand the complexity of the problem, let's analyze an extremely simple model: a robot with a single, freely rotating wheel. In this frame, assuming rotary encoders on the wheel, we can think about measuring directly the wheel speed and integrate these measurement to get the travelled distance, and measure the variation in the orientation of the wheel to get the position. But actually there are a lot of imperfection that can lead to error, for example:
	\begin{itemize}
		\item wheel can be non perfectly perendicular to the ground
		\item the friction between the floor and the wheel might not be enough to avoid slippage (expecially )
		\item there is no such thing as a perfect sensor, so the use of encoders introduce an error
	\end{itemize}
Even if all these concurrent causes seem negligible, you have to take into account that the errors sum up over time, so an error of a few millimeters per meter might become significant over time. \\ 
Moreover, the probability of slippage gets higher in systems with more than one wheel, because, because for a system with multiple wheels to move without slippage, a point must exists around which all the wheels can move along a circular path. This point is called \ac{ICC} (see Figure \ref{fig:icc}), and can be easily identified by looking for the intersection of the axis of all wheels. If the intersaction exists in a single point, it's called the \ac{ICC}.
But even if the odometry estimated from the wheels is not a good solution if used alone, it can be used as a starting point for other, more complex, method. For this reason we are now going to descript how to estimate the odometry starting from the wheels encoders in our specific robot. This computation is different according to the \textbf{motion model} of the considered robot. As we'll see in Section \ref{sec:grapeHwArch}, the robot we used is the Husky platform (see Figure \ref{fig:husky}) from Clearpath Robotics, that moves with a  \textit{skid steering} kinematics, that is a derivative of \textit{differential drive} kinematics. Thus, we're going to describe these two motion model with more detail.


\begin{figure}
	\centering
	\includegraphics[width=0.8\textwidth]{Images/background_and_tools/icc.png}
	\caption{\textit{If the axis of all the wheels intersect in a single point, it's called ICC and the robot can move without slipping}}
	\label{fig:icc}
\end{figure}


\begin{figure}
	\centering
	\subfloat[]{%
		\includegraphics[width=0.55\textwidth]{Images/background_and_tools/diffDrive.png}
		\label{fig:diffDriveA}}
	\qquad
	\subfloat[]{%
		\includegraphics[width=0.3\textwidth]{Images/background_and_tools/diffDriveExample.jpg}
		\label{fig:diffDriveB}}
	\caption{\textit{On figure \ref{fig:diffDriveA}, the scheme of a differential drive motion model; in figure \ref{fig:diffDriveB}, an example of a differential drive robot (Pioneer 3DX).}}
	\label{fig:diffDrive}
\end{figure}

\subsection{Differential drive robot}
In a differential drive system, the movement of the robot is only based on two separately driven wheels, placed on either side of the robot, on the same axis (see Figure \ref{fig:diffDrive}), and optionally a central, non-actuated caster wheel for stability. The two side wheels are not steerable, so the changes of direction are realized through application of different speed to the two wheels. For example, intuitively, if the wheels move at the same speed and in the same direction, the robot will move straight; if the wheels move at the same speed but different directions, the robot rotates in place.
 By recalling the definition of \ac{ICC}, we observe that, if the wheels are correctly aligned, a differential drive robot always have a well-defined \ac{ICC} and the slippage of the wheels is not very accentuated. \\


At each instant in time, since the \ac{ICC} is well-defined, both the left and right wheel follow a path that moves around \ac{ICC} at the same angular speed $\omega$, and thus:

\begin{empheq}[left=\empheqlbrace]{align}
\omega (R + \frac{L}{2}) = v_r  \label{eq:diffDrive1}\\ 
\omega (R - \frac{L}{2}) = v_l   \label{eq:diffDrive2}
\end{empheq}

where $L$ is the distance between the center of the two wheels, $v_r$ and $v_l$ are, respectively, the linear velocity of the right and left wheel, $R$ is the signed distance between the \ac{ICC} and the midpoint of the wheels. Note that the only parameter constant through time is $L$, since it's a physical property of the robot structure, while all other parameter evolve during the movement. \\
By combining \ref{eq:diffDrive1} and \ref{eq:diffDrive2}, we get:

\begin{empheq} {align}
R=\frac{L}{2} \frac{(v_r+v_l)}{(v_r-v_l)},  \qquad \omega=\frac{(v_r-v_l)}{L}
\end{empheq}

Observing these results we can validate the intuitive impressions about particular cases made a few lines above:
\begin{itemize}
	\item if $v_r=v_l$, the curvature radius is infinite, because the robot is moving straight.
	\item if $v_r=-v_l$, the robot is moving around the midpoint of the wheels
\end{itemize}
We give now some details about odometry computation. Let's assume, in a certain moment $t=t_0$, that the robot pose is $(x,y,\theta)$. We assume that in the time interval $t_0\rightarrow (t_0 +\delta t$) the values $v_r$ and $v_l$ are constant; if we observe figure \ref{fig:diffDriveDeltaT} under these condition, we have:
\begin{equation}
	ICC=(x-Rsin\theta, y+Rcos\theta)
\end{equation}
Write now the expressions for $(x',y',\theta')$:

\[
\begin{bmatrix}
\dot{x} \\
\dot{y} \\
\dot{\theta}
\end{bmatrix}
 = 
\begin{bmatrix}
cos(\omega \delta t) & -sin(\omega \delta t) & 0 \\
sin(\omega \delta t)  & cos(\omega \delta t) & 0 \\
0                                       & 0                                      & 1
\end{bmatrix}
\begin{bmatrix}
x-ICC_x\\
y-ICC _y\\
\theta
\end{bmatrix}
+
\begin{bmatrix}
ICC_x \\
ICC_y \\
\omega \delta t
\end{bmatrix}
\]

With this procedure we have identified 3 of the elements of the target tuple (see expression \ref{eq:odometryTuple}), but we still have to retrieve $x$,$y$ and $\theta$. For this reason we consider that by assuming an initial pose ($x_0, y_0, \theta_0$), knowing that:
\begin{equation}
	V(t)=\frac{(v_r+v_l)}{2}
\end{equation}
where $V(t)$ represent the complexive speed of the robot, and assuming to know the functions $v_r(t)$ and $v_l(t)$ \textit{i.e.} the linear speed of the wheel in time, we can calculate $(x,y,\theta)$ by integrating the speed of the robot over time, that is:

\begin{empheq}{align}
	x(t)            & = \int_{0}^{t}V(t)cos(\theta(t))dt \\
	y(t)            & = \int_{0}^{t}V(t)sin(\theta(t))dt \\
	\theta(t)  & = \int_{0}^{t}\omega(t)d\omega
\end{empheq}

and, in our specific case we can write it as function of $v_l$ and $v_r$, that are the quantities that are directly measured on the wheels.

\begin{empheq}{align}
	x(t)            & = \frac{1}{2}\int_{0}^{t}(v_r(t)+v_l(t))cos(\theta(t))dt \\
	y(t)            & = \frac{1}{2}\int_{0}^{t}(v_r(t)+v_l(t))sin(\theta(t))dt \\
	\theta(t)  & =\frac{1}{L} \int_{0}^{t}(v_r(t)-v_l(t))(t)
\end{empheq}
So the tuple required by the odometry calculation is now complete.

\begin{figure}
	\centering
	\includegraphics[width=0.8\textwidth]{Images/background_and_tools/diffDriveDeltaT.png}
	\caption{\textit{Differential drive: $(x,y,\theta) \rightarrow (x',y',\theta')$}}
	\label{fig:diffDriveDeltaT}
\end{figure}

\subsection{Skid-steering}

However, observing image \ref{fig:husky}, it's very easy to contest that the Husky platform used in \ac{GRAPE} project is not similar to the model described in previous section, because the number of actuated wheels is four instead of two. However, the motion model of the Husky is more similar to the differential drive because:
\begin{itemize}
	\item wheels are not steerable
	\item being $v_{f\star}$ the speeds of the front wheels, $v_{r\star}$ the speeds of the rear wheels, $v_{\star r}$ the speeds of the right wheels, $v_{\star l}$ the speeds of the left wheels, we always have:
	\begin{empheq}{align}
	v_{fr}=v_{rr} \\
	v_{fl}=v_{rl}
	\end{empheq}
\end{itemize}


\begin{figure}
	\centering
	\includegraphics[width=0.6\textwidth]{Images/background_and_tools/husky.png}
	\caption{\textit{Husky platform from Clearpath Robotics is the platform used for the development of \ac{GRAPE} project.}}
	\label{fig:husky}
\end{figure}

This type of motion model is called \textit{skid steering}, and is often use in real-world applications (see figure \ref{fig:skidSteeringB}) because of its simple and robust mechanical structure that leaving more room in the vehicle for the mission equipment. In
addition, it has good mobility on a variety of terrains, which makes it suitable
for all-terrain missions. But, of course, it also present a variety of problems and weakness. For example, if you recall what we told about \ac{ICC} in Section \ref{sec:odometry}, it's clear that the \ac{ICC} of such a model will always be at the infinite, since wheels are organized in 2 parallel rows that cannot steer. Thus, skid steering robots can't turn without slipping of the wheels! 
From a theoretical point of view, the main consequence is a a complexity in obtaining an accurate kinematics and dynamic model of \textit{skid steering} \parencite{skidSteeringDifficult}. On the other hand, in the context of \ac{GRAPE} project this leads to two major practical problems:
\begin{itemize}
	\item the slippage of the robot leads to higher power consuption of the electric engines with respect to the a system with explicit steering \parencite{skidSteeringConsumption}. This is something to be taken into account in the sizing phase of the power system of the robot.
	\item the estimation of the odometry is going to be much more imprecise than in the differential drive case, for the error introduced by the slipping of the wheels. 
\end{itemize}

\begin{figure}
	\centering
	\subfloat[]{%
		\includegraphics[width=0.5\textwidth]{Images/background_and_tools/skidSteeringModel.png}
		\label{fig:skidSteeringA}}
	\qquad
	\subfloat[]{%
		\includegraphics[width=0.35\textwidth]{Images/background_and_tools/muletto.jpg}
		\label{fig:skidSteeringB}}
	\caption{\textit{On figure \ref{fig:skidSteeringA}, the scheme of a skid steering motion model; in figure \ref{fig:skidSteeringB}, an example of a skid steering vehicle.}}
	\label{fig:skidSteering}
\end{figure}

Actually, there is a quite simple way  \parencite{skid2diff} to model with an accettable approximation the skid steering as an equivalent differential drive system, where the distance between the wheels is obtained multiplying the original distance for a factor $\chi$ that is function of the physical structure of the robot

\begin{figure}
	\centering
	\includegraphics[width=0.4\textwidth]{Images/background_and_tools/diffSkidEquivalence.png}
	\caption{\textit{The equivalence of differential drive motion model and skid steering model according to \cite{skid2diff}}}
	\label{fig:husky}
\end{figure}

This approximation relies on some assumptions:
\begin{itemize}
	\item the mass center of the robotis located at the geometric center of the body frame.
	\item the two wheels of each side rotate at the same speed.
	\item \textbf{the robot is running on a firm ground surface, and four wheels are always in contact with the ground surface.}
\end{itemize}
We cannot guarantee that conditions $1)$ and $2)$ will hold in our context, but we are almost sure that condition $3)$ is \textbf{not} going to be verified, given the condition in which our robot is going to operate (vineyard terrain), so this is another reason for the wheel odometry not to be very precise. Thus, the integration of several sensors beyond the wheels encoders is essential to correct the wheel odometry.

\section{Sensor fusion}\label{sec:sensorFusion}

First of all, clarify what we mean for sensor fusion, following the definition of  \cite{sensorFusionDef}:
\blockquote{\textit{
"Sensor fusion is the combining of sensory data or data derived from sensory
data in order to produce enhanced data in form of an internal representation
of the process environment. The achievements of sensor fusion are
robustness, extended spatial and temporal coverage, increased confidence,
reduced ambiguity and uncertainty, and improved resolution."}}
Let's analyze this last list, element by element, recalling the analysis of \cite{sensorFusionAdvantages}:
\begin{itemize}
	\item \textit{robustness}: redundancy generated by the presence of multiple sensors make the robot more resistant to partial failures
	\item \textit{extended spatial and temporal coverage}: disseminate sensors are more likely to measure the dimension concerned with temporal continuity and from several positions
	\item \textit{increased confidence}: measures confirmed by more that one sensor are given a larger weight
	\item \textit{reduced ambiguity and uncertainty}: joint information reduces the set
of ambiguous interpretations of the measured value
	\item \textit{robustness against interference}:  by increasing the dimensionality of
the measurement space (\textit{e.g.}, measuring the desired quantity with optical sensors and ultrasonic sensors) the system becomes less vulnerable
against interference.
\end{itemize}

\begin{figure}
	\centering
	\subfloat[]{%
		\includegraphics[width=0.3\textwidth]{Images/background_and_tools/sensorFusionScheme.png}
		\label{fig:sensorFusionScheme}}
	\qquad
	\subfloat[]{%
		\includegraphics[width=0.3\textwidth]{Images/background_and_tools/multiSensorScheme.png}
		\label{fig:multiSensorScheme}}
	\caption{\textit{Sensor fusion \ref{fig:sensorFusionScheme} vs multi-sensor integration \ref{fig:multiSensorScheme}}}
	\label{fig:fusionVsMultiSensor}
\end{figure}

Even if the name could be misleading, sensor fusion is a different from \textit{multi-sensor integration} in the sense that multi-sensor integration only consists in the simultaneous use of disparate sensor sources in order to accomplish a goal task. Sensor fusion tecniques make a step further, and aim to the construction of a single common representation, using all the available sensor sources. The difference betweeen multi-sensor integration and sensor fusion is described graphically in image \ref{fig:fusionVsMultiSensor}, to underline that sensor fusion actually provides one single representation of the state of the environment that is used as a single input stream by the control application, while in multi-sensor integration the application uses each of the sensor data streams as direct input. In this context, we are assuming the definition of environment, from \cite{stateOfEnvironment}: pose of the robot, 	velocity of the robot, and velocity of the joints, configuration of actuators, location and features of surrounding objects, location and velocity of moving object and people.

In \ac{ROS} ecosystem, several open source solutions exist that implements \textit{sensor fusion-based} odometry; we tried two of them, which gave proof of good functioning in the past: \textbf{ROAMFREE} (\cite{roamfreePaper}, \cite{roamfreeUtilizzo1}, \cite{roamfreeUtilizzo2}), a graph-based algorithm, and \textbf{Robot Localization} (\cite{robotLocalizationPaper}, \cite{robotLocalizationUtilizzo1}, \cite{robotLocalizationUtlizzo2}), based on Kalman filters.

After the experimental campaign in Casciano Terme, ROAMFREE turned out to be harder to configure for our specific problem, while Robot Localization had some specific documentation about the its usage in the required context\footnote{http://docs.ros.org/kinetic/api/robot\_localization/html/integrating\_gps.html},
so we opted for this last one. Further details about our usage or Robot Localization will be given in Chapter \ref{chap:localization}, while we are giving a few hints about its general functioning.

\subsection{Robot Localization}

Citing Robot Localization documentation\footnote{http://docs.ros.org/kinetic/api/robot\_localization/html/}:
\blockquote{\textit{"Robot\_localization is a collection of state estimation nodes, each of which is an implementation of a nonlinear state estimator for robots moving in 3D space. It contains two state estimation nodes, ekf\_localization\_node and ukf\_localization\_node". In addition, robot\_localization provides navsat\_transform\_node, which aids in the integration of GPS data."}}

\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{Images/background_and_tools/robotLocalizationLogo.png}
	\caption{\textit{A scheme of Robot Localization senor fusion}}
	\label{fig:robotLocalizationDisegno}
\end{figure}

The Robot Localization algorithm is implemented specifically for \ac{ROS}, and it's widely adopted by \ac{ROS} community for its documentation, and its easiness of use and configuration. It fuses an unlimited number of sensor sources, for each of which you can specify the message fields to be fused in the global estimate. \\

The state variables considered by Robot Localization are:
\begin{equation}
	<x, y, z, \psi, \theta, \phi, \dot{x}, \dot{y}, \dot{z},\dot{\psi}, \dot{\theta}, \dot{\phi}, \ddot{x}, \ddot{y}, \ddot{z}>
	\label{eq:robotLocalizationState}
\end{equation}
where $ \psi, \theta, \phi$ correspond to the euler angles \textit{roll, pitch, yaw}. 
The node accepts as inputs several types of \ac{ROS} messages:
\begin{itemize}
	\item \textit{nav\_msgs/Odometry}: the odometry estimation output by another node can be used entirely as input; this is the case of the odometry estimated only using wheels encoders
	\item \textit{sensor\_msgs/Imu}: the output of an \ac{IMU} sensor
	\item \textit{geometry\_msgs/PoseWithCovarianceStamped}: an estimate of the position and orientation of the robot, together with the timestamp of the measure and its covariance matrix
	\item \textit{geometry\_msgs/TwistWithCovarianceStamped}: an estimate of the linear and angular velocities of the robot, together with the timestamp of the measure and its covariance matrix.
\end{itemize}


% TODO:AMCL se lo useremo


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Navigation Stack}\label{sec:navigationStack}
SKETCH:
\begin{itemize}
	\item cos'è navigation stack (immagine http://wiki.ros.org/move\_base?action=AttachFile\&do=get\&target=overview\_tf.png)
	\item local/global costmap
\end{itemize}